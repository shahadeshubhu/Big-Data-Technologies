{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"11zBxdRUoQwvFIFZwqjxoN-SZqZlhfn-b","timestamp":1758569244939}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# COMP4124 Lab 05: MLlib"],"metadata":{"id":"_N-_I-nU46p0"}},{"cell_type":"markdown","source":["The aim of this lab is to get some practice with Spark's MLlib library.\n","\n","This notebook is split into two parts:\n","\n","* Part 1 gives an example of cross-validation with MLlib\n","* In Part 2, you will put together a pipeline for predicting appliance energy consumption"],"metadata":{"id":"E1_3PN_KWdoH"}},{"cell_type":"markdown","source":["## Set-Up"],"metadata":{"id":"wBc5444hIdrv"}},{"cell_type":"markdown","source":["**Install pyspark so can use within the notebook:**"],"metadata":{"id":"e_Rlxi3SshmM"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"U7Jfmp4yrseB"},"outputs":[],"source":["!pip install pyspark"]},{"cell_type":"markdown","source":["**Initialise the `SparkSession`:**"],"metadata":{"id":"2Q84rD4psy7k"}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","\n","spark = SparkSession \\\n","    .builder \\\n","    .master(\"local[*]\") \\\n","    .appName(\"Lab05\") \\\n","    .getOrCreate()\n","\n","sc = spark.sparkContext"],"metadata":{"id":"yaqM7kiBsPtZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_path = \"/content/drive/My Drive/COMP4124_notebooks/24-25/data/\""],"metadata":{"id":"rN4259fbX3pQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pyspark.sql.functions as F"],"metadata":{"id":"gvhDNaJHm6ge"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Part 1: Cross-Validation Example"],"metadata":{"id":"Mni7PSsVlBBr"}},{"cell_type":"markdown","source":["For this example, we are going to use the student marks data from the MLlib lecture.\n","\n","**Load the data, cast columns to the correct types and drop the columns we don't want to use:**"],"metadata":{"id":"gj4g3RFNmlYN"}},{"cell_type":"code","source":["marks_df = spark.read.csv(data_path+'results.csv', header=True)\n","marks_df = marks_df \\\n","  .select([F.col(c).cast('float').alias(c) for c in marks_df.columns]) \\\n","  .drop(*[c for c in marks_df.columns if 'Question' in c])\n","marks_df.show(5)"],"metadata":{"id":"irCOpUasmyUS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Split data into training and final test sets:**"],"metadata":{"id":"8zTfMYS4v58M"}},{"cell_type":"code","source":["seed = 202503\n","train_df, test_df = marks_df.randomSplit([0.7, 0.3], seed)"],"metadata":{"id":"vuQ1OBUrv9oA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### MLlib `CrossValidator`\n","\n","MLlib provides us with the `CrossValidator`. This lets us do model selection (hyperparameter tuning) to find the best model for a given task. The MLlib ML Tuning Guide is available [here](https://spark.apache.org/docs/latest/ml-tuning.html).\n","\n","**Note:** MLlib gives us another model selection `TrainValidationSplit`. We won't cover that in this lab, but feel free to try it out as part of your project, or just in your own time."],"metadata":{"id":"kMmpKQxqZ-OR"}},{"cell_type":"code","source":["from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"],"metadata":{"id":"Kw-1qeSz4GUY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To use `CrossValidator`, we need:\n","* an Estimator\n","* a parameter grid\n","* an Evaluator\n","\n","We will set up our Pipeline from the MLlib lecture, to use as the Estimator."],"metadata":{"id":"SsGxdtVJ4tAs"}},{"cell_type":"markdown","source":["**Set up instances of VectorAssembler and LinearRegression, then define our Pipeline stages:**"],"metadata":{"id":"I1ik3Vews2TR"}},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler\n","from pyspark.ml.regression import LinearRegression\n","from pyspark.ml import Pipeline"],"metadata":{"id":"Q3eeu-_ktBJC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["feature_cols = marks_df.columns\n","feature_cols.remove('Total')\n","feature_cols"],"metadata":{"id":"S90one71tqTt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n","lr = LinearRegression(featuresCol='features', labelCol='Total')\n","pipeline = Pipeline(stages=[assembler, lr])"],"metadata":{"id":"DcuEjeIRtr9_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**We also need to define a parameter grid, i.e. the parameters and their values that we want to investigate.** I am only using very few combinations for this example, so that it runs reasonably quickly."],"metadata":{"id":"YGEkqnyU4e-e"}},{"cell_type":"code","source":["paramGrid = ParamGridBuilder() \\\n","  .addGrid(lr.regParam, [0.5, 0.1]) \\\n","  .addGrid(lr.maxIter, [1, 5]) \\\n","  .build()"],"metadata":{"id":"K_DMl1NU4euL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Set up an Evaluator:** We will use the RegressionEvaluator from the MLlib lecture. We can use `getLabelCol()` and `getPredictionCol()` to get the relevant columns directly from our LinearRegressor, rather than having to specify them ourselves."],"metadata":{"id":"t2ybbb7jw108"}},{"cell_type":"code","source":["from pyspark.ml.evaluation import RegressionEvaluator"],"metadata":{"id":"cND-14l1xDSl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["evaluator = RegressionEvaluator(metricName=\"mae\", labelCol=lr.getLabelCol(), predictionCol=lr.getPredictionCol())"],"metadata":{"id":"KlawBlfFw9vq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Now we are ready to create our CrossValidator, using 3 folds:**"],"metadata":{"id":"nfG6iWsrxGup"}},{"cell_type":"code","source":["cv = CrossValidator(estimator=pipeline,\n","                    estimatorParamMaps=paramGrid,\n","                    evaluator=evaluator,\n","                    numFolds=3)"],"metadata":{"id":"WUqFW-IO5hAf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Run the cross-validation, fitting models to our training data:** This may take a while if you investigate many parameter combinations!"],"metadata":{"id":"7ZVgIRRsxlom"}},{"cell_type":"code","source":["cv_model = cv.fit(train_df)"],"metadata":{"id":"7AF2GF8O5kXS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["type(cv_model)"],"metadata":{"id":"AP6wkw33yJJh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["`cv_model` now contains the pipeline model with the highest average metric across the 3 folds.\n","\n","**We can use this model to make predictions on the final test set.**"],"metadata":{"id":"Gz3Etnlkye7J"}},{"cell_type":"code","source":["predictions = cv_model.transform(test_df)"],"metadata":{"id":"qzomn4Z4y3aO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Evaluate how well the model has done on the final test set:"],"metadata":{"id":"zOI2S8eCy8tp"}},{"cell_type":"code","source":["evaluator.evaluate(predictions)"],"metadata":{"id":"XXoddyazzTPn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Investigate the best pipeline and its stages:**\n","\n","We can use the attribute `bestModel` to access the best pipeline. We can then get a list of stages in this best pipeline via the attribute `stages`."],"metadata":{"id":"AO9UmdKRzz9-"}},{"cell_type":"code","source":["best_pipeline = cv_model.bestModel\n","best_pipeline.stages"],"metadata":{"id":"mEnEzHe1zdax"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can then look at the best parameters found from the combinations we defined in our parameter grid. What were the best LinearRegression parameters found?"],"metadata":{"id":"A7ANhjqT0HHl"}},{"cell_type":"code","source":["best_lr = best_pipeline.stages[1]\n","best_lr_params = best_lr.extractParamMap()\n","best_lr_params"],"metadata":{"id":"rKvR7mgl0GNJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["If we wanted to just look at the parameters we investigated (`regParam` and `maxIter`):"],"metadata":{"id":"Xscksny10rmw"}},{"cell_type":"code","source":["{(k.name,v) for (k,v) in best_lr_params.items() if k.name in ['regParam','maxIter']}"],"metadata":{"id":"YBGUiMXN0wwP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Part 2: Appliance Energy Prediction"],"metadata":{"id":"dRSzYOIR1_fc"}},{"cell_type":"markdown","source":["The aim of this section is to use Spark MLlib to create a pipeline that preprocesses a dataset, trains a model, and makes predictions on a final test set. We also want to use hyperparameter tuning (via cross-validation) in order to fine-tune our model.\n","\n","The content of this lab has been inspired by the sample provided in the [Databricks documentation](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/2854662143668609/2084788691983918/6837869239396014/latest.html).\n","\n","**Data Information**: This dataset contains energy consumption data from appliances at a 10-minute resolution for about 4.5 months. The house temperature and humidity conditions were monitored with a wireless sensor network. Each wireless node transmitted the temperature and humidity conditions around every 3.3 min. Then, the wireless data was averaged for 10-minute periods. The energy data was logged every 10 minutes. Weather from the nearest airport weather station (Chievres Airport, Belgium) was also logged. This dataset is from [Candanedo et al](http://dx.doi.org/10.1016/j.enbuild.2017.01.083) and is hosted by the UCI Machine Learning Repository. [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets/Appliances+energy+prediction).\n","\n","**Goal**: We want to learn to predict appliances' energy consumption based on weather information. It would also be nice to know which input features are the most relevant to make predictions. This is a regression problme."],"metadata":{"id":"zJdPAvBg2V5g"}},{"cell_type":"markdown","source":["### Load and understand the dataset"],"metadata":{"id":"GD7WAghY3q9o"}},{"cell_type":"code","source":["energy_df = spark.read.csv(data_path+'energydata_complete.csv', header=True)\n","print(f'Number of rows: {energy_df.count()}')\n","energy_df.show(5)\n","energy_df.printSchema()"],"metadata":{"id":"nnnVD-8q3s7D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Cache the dataset:"],"metadata":{"id":"9LcKYF4o4WhQ"}},{"cell_type":"code","source":["energy_df.cache()"],"metadata":{"id":"vzqg9Nux4YA6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Data description"],"metadata":{"id":"T5yuHwlu8kv1"}},{"cell_type":"markdown","source":["From the UCI ML Repository description, we know that the columns have the following meanings.\n","\n","**Attribute information**:\n","```\n","date, year-month-day hour:minute:second\n","Appliances, energy use in Wh\n","lights, energy use of light fixtures in the house in Wh\n","T1, Temperature in kitchen area, in Celsius\n","RH_1, Humidity in kitchen area, in %\n","T2, Temperature in living room area, in Celsius\n","RH_2, Humidity in living room area, in %\n","T3, Temperature in laundry room area\n","RH_3, Humidity in laundry room area, in %\n","T4, Temperature in office room, in Celsius\n","RH_4, Humidity in office room, in %\n","T5, Temperature in bathroom, in Celsius\n","RH_5, Humidity in bathroom, in %\n","T6, Temperature outside the building (north side), in Celsius\n","RH_6, Humidity outside the building (north side), in %\n","T7, Temperature in ironing room , in Celsius\n","RH_7, Humidity in ironing room, in %\n","T8, Temperature in teenager room 2, in Celsius\n","RH_8, Humidity in teenager room 2, in %\n","T9, Temperature in parents room, in Celsius\n","RH_9, Humidity in parents room, in %\n","To, Temperature outside (from Chievres weather station), in Celsius\n","Pressure (from Chievres weather station), in mm Hg\n","RH_out, Humidity outside (from Chievres weather station), in %\n","Wind speed (from Chievres weather station), in m/s\n","Visibility (from Chievres weather station), in km\n","Tdewpoint (from Chievres weather station), Â°C\n","rv1, Random variable 1, nondimensional\n","rv2, Random variable 2, nondimensional\n","```\n","\n","**The target variable is the energy use of the Appliances.**\n","\n","For now, we will leave the two variables `rv1` and `rv2` in our dataset, to see if they are affecting much our methods, then we can try to remove them and see if we improve the results."],"metadata":{"id":"Yjv3oMLA8tbY"}},{"cell_type":"markdown","source":["#### Cast the columns to appropriate types"],"metadata":{"id":"uD8UhCE193t4"}},{"cell_type":"markdown","source":["Most columns in this dataset are numerical, so we can just cast them to floats. The exception is the 'date' column, which we instead want to cast to a timestamp.\n","\n","<font color='blue'>**Task:** Cast the 'date' column to a timestamp. Cast all other columns to floats. Assign the resulting DataFrame back to `energy_df`.<br>\n","There are a lot of columns, so you should do this programmatically rather than manually casting every column individually!</font>"],"metadata":{"id":"xTM-f-nF97Q-"}},{"cell_type":"code","source":["numerical_cols = energy_df.columns\n","numerical_cols.remove('date')\n","energy_df = energy_df \\\n","  .select(['date']+[F.col(c).cast(\"float\").alias(c) for c in numerical_cols]) \\\n","  .withColumn('date',F.to_timestamp(F.col('date')))\n","\n","energy_df.printSchema()"],"metadata":{"id":"fryzPa5j-a9Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Data preprocessing"],"metadata":{"id":"DQiJtA9x8_gv"}},{"cell_type":"markdown","metadata":{"nbgrader":{"grade":false,"grade_id":"cell-74870c89983edfb6","locked":true,"schema_version":3,"solution":false,"task":false},"id":"gv1z5FhqkdK3"},"source":["This dataset is nicely prepared for Machine Learning and requires very little preprocessing. However, rather than keeping the date as a single timestamp, we would like to have some additional columns, including 'day of the year', 'hour', and 'month of the year'.\n","\n","<font color='blue'>**Task:** Add columns `dayofyear`, `hour` and `month` to the DataFrame. You will need to find the appropriate functions from the DataFrame API. Assign the resulting DataFrame back to `energy_df`.</font>"]},{"cell_type":"code","source":["energy_df = energy_df \\\n","  .withColumn(\"dayofyear\", F.dayofyear('date')) \\\n","  .withColumn(\"hour\", F.hour('date')) \\\n","  .withColumn(\"month\", F.month('date'))"],"metadata":{"id":"iVYpBZspADMV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<font color='blue'>**Task:** Drop the 'date' column - we don't want to use this in our predictions now we have our three new time-related columns added. Assign the resulting DataFrame back to `energy_df`.</font>"],"metadata":{"id":"6Sr_STIOAhxJ"}},{"cell_type":"code","source":["energy_df = energy_df.drop('date')\n","energy_df.show(5)"],"metadata":{"id":"CuvBDj_uAxRG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["energy_df.printSchema()"],"metadata":{"id":"V6UT55k8A35E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Data visualisation"],"metadata":{"id":"YR03WMldJJDT"}},{"cell_type":"markdown","source":["Before applying any machine learning algorithm, it is good practice to try to visualise your data. For example, we could see how much energy is consumed by appliances depending on the month.\n","\n","<font color='blue'>**Task:** Create a histogram of the total amount of energy used per month. Plot the histogram using a library of your choice (e.g. matplotlib).</font>"],"metadata":{"id":"S0Fi8iIfJJDU"}},{"cell_type":"code","source":["from matplotlib import pyplot as plt"],"metadata":{"id":"L4VbLVRhJJDU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["hist = energy_df.groupBy('month').sum('Appliances').sort(\"month\").toPandas()\n","hist"],"metadata":{"id":"y9xtWCX8JJDU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.bar(hist['month'], hist['sum(Appliances)'])\n","plt.title('Energy consumption per month')\n","plt.xlabel('Month')\n","plt.ylabel('Sum of Appliance Energy')\n","plt.show()"],"metadata":{"id":"174xDiggJJDV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<font color='blue'>**Open task:** Do any other analysis of the data you think would be interesting or useful!</font>"],"metadata":{"id":"bP8I5wKoJJDV"}},{"cell_type":"code","source":[],"metadata":{"id":"jSZ93lxeJJDV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Split into train and test sets"],"metadata":{"id":"Wn-T2BxmJMIz"}},{"cell_type":"markdown","source":["**Split the data into training and test sets:** We will train and tune our model on the training set, and then see how well we do on a final test set.\n","\n","<font color='blue'>**Task:** Split `energy_df` into 70% for training and 30% for the test set.</font>"],"metadata":{"id":"8T1LqWNqBTBw"}},{"cell_type":"code","source":["seed = 202503\n","train_df, test_df = energy_df.randomSplit([0.7, 0.3], seed)"],"metadata":{"id":"0M45JZJWBoSC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Create a Pipeline"],"metadata":{"id":"kNNH-kSVJiAz"}},{"cell_type":"markdown","source":["We are going to create a simple Pipeline, using only the following stages:\n","\n","* **VectorAssembler:** To combine all the input columns into a single vector column (i.e. all the columns apart from 'Appliances').\n","* **GBTRegressor:** The learning algorithm we will use is [Gradient-Boosted Trees](https://en.wikipedia.org/wiki/Gradient_boosting#Gradient_tree_boosting) (GBTs)."],"metadata":{"id":"six-xJn4Jm-V"}},{"cell_type":"markdown","source":["<font color='blue'>**Task:** Create a VectorAssembler. Remember not to include the label column 'Appliances' in the features!</font>"],"metadata":{"id":"GRud15uPKhQo"}},{"cell_type":"code","source":["features_cols = energy_df.columns\n","features_cols.remove('Appliances')\n","vector_assembler = VectorAssembler(inputCols=features_cols, outputCol=\"features\")"],"metadata":{"id":"DdKdfoqmKr6W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<font color='blue'>**Task:** Create a GBTRegressor. Don't indicate any parameters, other than specifying the class label (i.e. `labelCol`) to be `'Appliances'`.</font>"],"metadata":{"id":"9xWFvsFjLHUE"}},{"cell_type":"code","source":["from pyspark.ml.regression import GBTRegressor\n","\n","gbt = GBTRegressor(labelCol=\"Appliances\")"],"metadata":{"id":"NCnleudlKvQM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<font color='blue'>**Task:** Create a Pipeline, using your VectorAssembler and GBTRegressor as the two stages.</font>"],"metadata":{"id":"oaAhP0a-LmGY"}},{"cell_type":"code","source":["pipeline = Pipeline(stages=[vector_assembler, gbt])"],"metadata":{"id":"aKiWjZKKLsfV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Create a CrossValidator"],"metadata":{"id":"jf2NaFO-MLFZ"}},{"cell_type":"markdown","source":["<font color='blue'>**Task:** Define a parameter grid with the parameters you want to investigate. Don't attempt to do too many, otherwise it may take a very long time to run!<br>\n","You can look at the [GBTRegressor](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.regression.GBTRegressor.html) docs for the parameters you can choose.</font>"],"metadata":{"id":"D0R6owc0MUtf"}},{"cell_type":"code","source":["paramGrid = ParamGridBuilder() \\\n","  .addGrid(gbt.maxDepth, [5, 8]) \\\n","  .addGrid(gbt.maxIter, [10, 20]) \\\n","  .build()"],"metadata":{"id":"JvOCFSOPMN6-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<font color='blue'>**Task:** Create a RegressionEvaluator that uses the Root Mean Squared Error (RMSE) as the performance metric. Specify the `labelCol` and `predictionCol` by getting them from your GBTRegressor.</font>"],"metadata":{"id":"jvv1W6EGM5VH"}},{"cell_type":"code","source":["evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=gbt.getLabelCol(), predictionCol=gbt.getPredictionCol())"],"metadata":{"id":"942tOZkvND9Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<font color='blue'>**Task:** Create a CrossValidator that uses the Pipeline as an estimator, as well the parameter grid and evaluator that you defined above. Use 3 folds for the cross-validation.</font>"],"metadata":{"id":"wUrYBrqiOD-S"}},{"cell_type":"code","source":["cv = CrossValidator(estimator=pipeline, evaluator=evaluator, estimatorParamMaps=paramGrid, numFolds=3)"],"metadata":{"id":"Q7lf6fQjOOmS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<font color='blue'>**Task:** Run the cross-validation using the training data, to create a CrossValidatorModel called `cv_model`.</font>"],"metadata":{"id":"SL2-yz4QO-UC"}},{"cell_type":"code","source":["cv_model = cv.fit(train_df)"],"metadata":{"id":"uZqNNz_KPQ6d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Saving models:** MLlib lets you save models to disk. This means you can load those models directly from a file later on if needed, so you don't have to wait for them to train again. [See the docs for more information](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.tuning.CrossValidatorModel.html#pyspark.ml.tuning.CrossValidatorModel.save)"],"metadata":{"id":"700eHaMwPVVV"}},{"cell_type":"markdown","source":["### Evaluate the results"],"metadata":{"id":"6_pSz_m0QJ5f"}},{"cell_type":"markdown","source":["Let's see how the model does on the final test set!\n","\n","<font color='blue'>**Task:** Transform the test set to add a column with the predictions, using your CrossValidatorModel. Assign the resulting DataFrame to a variable called `predictions`.</font>"],"metadata":{"id":"S4MtH-WxQSYl"}},{"cell_type":"code","source":["predictions = cv_model.transform(test_df)"],"metadata":{"id":"AVHXSElhQowP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predictions.select('Appliances', 'prediction').show(5)"],"metadata":{"id":"z7o_nqIgQsTH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<font color='blue'>**Task:** Use your Evaluator that you defined above to calculate the RMSE.</font>"],"metadata":{"id":"c1pFQvkJQ8Uv"}},{"cell_type":"code","source":["evaluator.evaluate(predictions)"],"metadata":{"id":"2L-dEMh7RXTZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The result probably seems quite high. You can see if you can improve it!\n","\n","But first, let's see how we can find out the importance of the features from the GBT."],"metadata":{"id":"y2v3JKFzRlV0"}},{"cell_type":"markdown","source":["### Investigating the GBT"],"metadata":{"id":"Kjc4fXVwR3r0"}},{"cell_type":"markdown","source":["**Retrieve the best GBT from the CrossValidatorModel:**"],"metadata":{"id":"bXak4NXtR7d_"}},{"cell_type":"code","source":["best_pipeline = cv_model.bestModel\n","gbt_model = best_pipeline.stages[1]"],"metadata":{"id":"8eSNfed1R_zu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Check the feature importances from the GBT:**"],"metadata":{"id":"wtlGF-XlSk7l"}},{"cell_type":"code","source":["gbt_model.featureImportances"],"metadata":{"id":"tRKdlsXeSrJU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["A lot of these features are showing quite low importance. They won't be affecting the performance much as GBTs perform somewhat an implicit feature selection. But, we could make our model more interpretable if we remove the low importance features."],"metadata":{"id":"2E2dZ4YOVE7n"}},{"cell_type":"markdown","source":["<font color='blue'>**Task:** Create a list of the features with less than 0.05 in the feature importances. You will need to get the names of the feature columns the DataFrame.</font>"],"metadata":{"id":"7NWha3IvVaSh"}},{"cell_type":"code","source":["importances = gbt_model.featureImportances"],"metadata":{"id":"I1_IQSV4V-3D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["features_to_remove = []\n","for i in range(len(features_cols)):\n","  if importances[i] < 0.05:\n","    features_to_remove.append(features_cols[i])"],"metadata":{"id":"50WJozA9Vt3y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["features_to_remove"],"metadata":{"id":"yJQcRq0jWSA5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Remove the low importance features, and re-do the cross validation"],"metadata":{"id":"A19QljfMWbd_"}},{"cell_type":"markdown","source":["<font color='blue'>**Task:** Create a new VectorAssembler, with only higher importance features included. You don't need to modify your training and test DataFrames at all - you simply need to specify the relevant columns as the `inputCols` when creating the VectorAssembler.</font>"],"metadata":{"id":"lkCpdisrWjh9"}},{"cell_type":"code","source":["features_cols2 = energy_df.columns\n","features_cols2.remove('Appliances')\n","for feature in features_to_remove:\n","    features_cols2.remove(feature)\n","vector_assembler2 = VectorAssembler(inputCols=features_cols2, outputCol=\"features\")\n","features_cols2"],"metadata":{"id":"lKAPf05WWyUn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<font color='blue'>**Task:** Create a new Pipeline with your new VectorAssembler and your GBTRegressor instance.</font>"],"metadata":{"id":"_3R8VEcVXE7G"}},{"cell_type":"code","source":["pipeline2 = Pipeline(stages=[vector_assembler2, gbt])"],"metadata":{"id":"1Rprj1aVWiSw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<font color='blue'>**Task:** Re-do the cross-validation, using your new Pipeline as the estimator.</font>"],"metadata":{"id":"zPxoKu9HXdz7"}},{"cell_type":"code","source":["cv2 = CrossValidator(estimator=pipeline2, evaluator=evaluator, estimatorParamMaps=paramGrid, numFolds=3)"],"metadata":{"id":"xjP7z6aCXvUk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cv_model2 = cv2.fit(train_df)"],"metadata":{"id":"cA4GMlt8Xx_1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<font color='blue'>**Task:** Use your new model to make predictions, and then compute the RMSE.</font>"],"metadata":{"id":"EzntpD7mYSya"}},{"cell_type":"code","source":["predictions2 = cv_model2.transform(test_df)\n","rmse = evaluator.evaluate(predictions2)\n","print(rmse)"],"metadata":{"id":"G14EkEvfYaVu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In my case, I got about the same value for the RMSE, but this has achieved using far fewer features."],"metadata":{"id":"EV7hUM0hYppR"}},{"cell_type":"markdown","source":["<font color='blue'>**Task:** Have a look at the feature importances of the new model.</font>"],"metadata":{"id":"mo7UhA-3Y0eU"}},{"cell_type":"code","source":["cv_model2.bestModel.stages[1].featureImportances"],"metadata":{"id":"EAwNwEPRY8XY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Improving the model further\n","\n","<font color='blue'>**Open task:** There might be many ways to improve the results here. Implement some alternatives below.<br>\n","Some ideas for you to consider:\n","* Hyperparameter tuning: You probably initially used quite a small set of parameters in the cross-validation. Try different combinations of parameters and values.\n","* Alternative models: Try with different regression models from MLlib.\n","* Additional preprocessing: Is there any normalisation that may be needed? Are there outliers or noise that may have an impact on results?\n","</font>"],"metadata":{"id":"Em-BWHXzZE75"}},{"cell_type":"code","source":[],"metadata":{"id":"v1IX81KeZzmO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**It is good practice to stop the underlying SparkContext when we are done**"],"metadata":{"id":"mbTKvpJpJZiO"}},{"cell_type":"code","source":["spark.stop()"],"metadata":{"id":"ez4lU8yh6T68"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["spark"],"metadata":{"id":"ZW7PqtW16onp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"h0euCcYOFXOI"},"execution_count":null,"outputs":[]}]}