# Big Data Learning and Technologies

This repository contains coursework, practicals, and notes from the Big Data Learning and Technologies module, part of my MSc in Data Science at the University of Nottingham.

## Module Overview

This module introduces the fundamental concepts and tools for working with large-scale data in distributed computing environments. The focus is on both the theoretical foundations and the practical implementation of scalable data processing and machine learning systems.

## Core Concepts Covered

- Characteristics of Big Data: volume, velocity, variety
- Scale-out vs. scale-up architectures
- Data locality and distributed computing
- The MapReduce programming paradigm

## Technologies and Frameworks

- Hadoop and HDFS – Distributed storage and batch processing
- Apache Spark – Scalable data processing engine
- PySpark – Python API for Spark
- MLlib – Scalable machine learning library for Spark

## Example Topics

- Word count with MapReduce
- Spark DataFrame operations
- Handling large-scale feature engineering
- Building ML pipelines with MLlib
- Performance considerations in distributed computing

## Tools Used

- Python 3.x
- Apache Spark (via PySpark)
- Jupyter Notebooks
- Hadoop HDFS (via university setup or simulated environment)

## Learning Outcomes

By the end of this module, I was able to:

- Understand and explain the architecture of distributed systems
- Develop and optimize PySpark applications for large-scale data
- Implement scalable ML solutions using Spark MLlib
- Combine big data principles with practical coding skills to solve real-world problems
